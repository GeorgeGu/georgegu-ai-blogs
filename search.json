[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": " Notes from reading research papers and studies of projects",
    "section": "",
    "text": "phi-1 ‚Äì Textbooks Are All You Need\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "George Gu",
    "section": "",
    "text": "üëã Greetings! My name is George Gu, and I am currently an AI architect at Modernizing Medcine. I hold a M.Sc. degree in computer science from Tsinghua University In this blog, I would like to share some of my works in LLM and Alignments.\nLet‚Äôs build a better open community of AI\nIf you want to contact me, please feel free to send an email at this address.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "notes/Large Language Models/phi1.html",
    "href": "notes/Large Language Models/phi1.html",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "",
    "text": "Tip\n\n\n\nIt introduces phi-1, a model with 1.3B parameters that obtains a pass@1 rate of 50.6% on HumanEval thanks to a novel training process.\nüìù Paper: https://arxiv.org/pdf/2306.11644.pdf\nThe authors argue that high quality data can change the shape of the scaling laws, allowing small models to match the performance of bigger ones.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "phi-1 ‚Äì Textbooks Are All You Need"
    ]
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#the-importance-of-high-quality-data",
    "href": "notes/Large Language Models/phi1.html#the-importance-of-high-quality-data",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "The importance of high-quality data",
    "text": "The importance of high-quality data\n\nMotivation: The authors observe that standard code datasets like The Stack, StackOverflow and CodeContests suffer from several drawbacks: samples are not self-contained but referenced, a lot of them are trivial while the most complex ones are poorly documented, and the overall distribution is skewed towards certain topics and use cases.\nThey train their solution (phi-1) on a new dataset (&lt;7B tokens) as follows:\n\nPretraining on CodeTextbook, comprised of a filtered version of The Stack and StackOverflow(~6B tokens) + a synthetic samples generated by GPT-3.5 (&lt;1B tokens)\nFine-tuning on CodeExercises, a small synthetic dataset of Python exercises and solutions also generated by GPT-3.5 (~180M tokens)\n\n\nFiltering code\nThe authors use the Python subset of the deduplicated version of The Stack and StackOverflow (35B tokens). They used GPT-4 to annotate the quality of a subset (100k samples), using the following prompt: ‚Äúdetermine its educational value for a student whose goal is to learn basic coding concepts.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nThis prompt could probably be improved by asking GPT-4 to break down its reasoning into steps before outputting the final value.\n\n\nThis creates a dataset of code snippets and corresponding values. The authors produce embeddings of each code snippet using a pretrained CodeGen model, and train a random forest classifier to predict the quality of each sample.\n\n\nGenerating synthetic data\nThe authors argue the synthetic samples should be diverse (concepts, skills, scenarios, difficulty, complexity, style) and non-repetitive to reduce the risk of overfitting/memorizing and be more robust. Inspired by TinyStories, they use randomized seeds √† la Alpaca to generate samples with GPT-3.5:\n\nSynthetic training data (&lt;1B tokens): code and text with examples and constraints.\nCodeExercises (~180M tokens): Python exercises and solutions, where each exercise is a docstring of a function that needs to be completed.\n\n\n\nModel architecture and training\nphi-1 is a decoder-only transformer using rotary position embedding, FlashAttention and multi-head attention (MHA), with parallel MHA and MLP layers + codegen-350M-mono‚Äôs tokenizer.\n\n\n\n\n\n\nTip\n\n\n\nIts architecture is very much inspired by CodeGen and does not include Fill-In-the-Middle or Multi-Query-Attention like StarCoder (low-hanging fruit improvement).\n\n\nHyperparameters for phi-1 with 1.3B/350M parameters:\n\n24/20 layers\nHidden dimension = 2048/1024\nMLP-inner dimension = 8192/4096\n32/16 attention heads with dimension = 64/32\nSequence length = 2048\nObjective = next-token prediction",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "phi-1 ‚Äì Textbooks Are All You Need"
    ]
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#the-importance-of-fine-tuning",
    "href": "notes/Large Language Models/phi1.html#the-importance-of-fine-tuning",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "The importance of fine-tuning",
    "text": "The importance of fine-tuning\n\nFine-tuning phi-1 on CodeExercises greatly improves the model‚Äôs performance, even for tasks that are not in the fine-tuning dataset.\nThe authors notice that the model gets better at interpreting questions and logical relationships in the prompts. Interestingly, it also becomes better at using external libraries, even when they do not appear in the fine-tuning set (e.g., Pygame and Tkinter).",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "phi-1 ‚Äì Textbooks Are All You Need"
    ]
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#performance-evaluation",
    "href": "notes/Large Language Models/phi1.html#performance-evaluation",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nThe authors argue HumanEval‚Äôs binary score (code passes the unit tests, or it fails) does not capture the nuances of the model‚Äôs performance. They introduce an LLM grading using GPT-4 (between 0 and 10), which does not require tests.\n\nThere‚Äôs a concern that CodeExercises might contain samples that are similar to exercises in HumanEval. The authors propose to remove these samples and retrain phi-1 on this decontaminated set.\nThey report no meaningful n-gram overlap between CodeExercises and HumanEval (4 false positives). They then use a combination of embedding and syntax-based distances to find similar code snippets:\n\nSemantics: They use the L2 distance between embeddings produced by CodeGen\nSyntax: They calculate the (string) edit distance between the abstract syntax trees (ASTs) of two code snippets.\n\nDespite this data pruning, the authors claim that phi-1 still outperforms StarCoder on HumanEval.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "phi-1 ‚Äì Textbooks Are All You Need"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook\n\n\nA practical introduction to LLM fine-tuning\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJul 24, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "",
    "text": "With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models, including Alpaca, Vicuna, WizardLM, among others. This trend encouraged different businesses to launch their own base models with licenses suitable for commercial use, such as OpenLLaMA, Falcon, XGen, etc. The release of Llama 2 now combines the best elements from both sides: it offers a highly efficient base model along with a more permissive license.\nDuring the first half of 2023, the software landscape was significantly shaped by the widespread use of APIs (like OpenAI API) to create infrastructures based on Large Language Models (LLMs). Libraries such as LangChain and LlamaIndex played a critical role in this trend. Moving into the latter half of the year, the process of fine-tuning (or instruction tuning) these models is set to become a standard procedure in the LLMOps workflow. This trend is driven by various factors: the potential for cost savings, the ability to process confidential data, and even the potential to develop models that exceed the performance of prominent models like ChatGPT and GPT-4 in certain specific tasks.\nIn this article, we will see why instruction tuning works and how to implement it in a Google Colab notebook to create your own Llama 2 model. As usual, the code is available on Colab and GitHub.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "üîß Background on fine-tuning LLMs",
    "text": "üîß Background on fine-tuning LLMs\n\n\n\n\nLLMs are pretrained on an extensive corpus of text. In the case of Llama 2, we know very little about the composition of the training set, besides its length of 2 trillion tokens. In comparison, BERT (2018) was ‚Äúonly‚Äù trained on the BookCorpus (800M words) and English Wikipedia (2,500M words). From experience, this is a very costly and long process with a lot of hardware issues. If you want to know more about it, I recommend reading Meta‚Äôs logbook about the pretraining of the OPT-175B model.\nWhen the pretraining is complete, auto-regressive models like Llama 2 can predict the next token in a sequence. However, this does not make them particularly useful assistants since they don‚Äôt reply to instructions. This is why we employ instruction tuning to align their answers with what humans expect. There are two main fine-tuning techniques:\n\nSupervised Fine-Tuning (SFT): Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels.\nReinforcement Learning from Human Feedback (RLHF): Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using PPO), which is often derived from human evaluations of model outputs.\n\nIn general, RLHF is shown to capture more complex and nuanced human preferences, but is also more challenging to implement effectively. Indeed, it requires careful design of the reward system and can be sensitive to the quality and consistency of human feedback. A possible alternative in the future is the Direct Preference Optimization (DPO) algorithm, which directly runs preference learning on the SFT model.\nIn our case, we will perform SFT, but this raises a question: why does fine-tuning work in the first place? As highlighted in the Orca paper, our understanding is that fine-tuning leverages knowledge learned during the pretraining process. In other words, fine-tuning will be of little help if the model has never seen the kind of data you‚Äôre interested in. However, if that‚Äôs the case, SFT can be extremely performant.\nFor example, the LIMA paper showed how you could outperform GPT-3 (DaVinci003) by fine-tuning a LLaMA (v1) model with 65 billion parameters on only 1,000 high-quality samples. The quality of the instruction dataset is essential to reach this level of performance, which is why a lot of work is focused on this issue (like evol-instruct, Orca, or phi-1). Note that the size of the LLM (65b, not 13b or 7b) is also fundamental to leverage pre-existing knowledge efficiently.\nAnother important point related to the data quality is the prompt template. Prompts are comprised of similar elements: system prompt (optional) to guide the model, user prompt (required) to give the instruction, additional inputs (optional) to take into consideration, and the model‚Äôs answer (required). In the case of Llama 2, the authors used the following template for the chat models:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\nThere are other templates, like the ones from Alpaca and Vicuna, and their impact is not very clear. In this example, we will reformat our instruction dataset to follow Llama 2‚Äôs template. For the purpose of this tutorial, I‚Äôve already done it using the excellent timdettmers/openassistant-guanaco dataset. You can find it on Hugging Face under the name mlabonne/guanaco-llama2-1k. In the following, we will use a base model instead of a chat model, so this step is optional. Note that you don‚Äôt need to follow a specific prompt template if you‚Äôre using the base Llama 2 model instead of the chat version.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "ü¶ô How to fine-tune Llama 2",
    "text": "ü¶ô How to fine-tune Llama 2\nIn this section, we will fine-tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a T4 only has 16 GB of VRAM, which is barely enough to store Llama 2-7b‚Äôs weights (7b √ó 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see this excellent article for more information). This means that a full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\nTo drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we‚Äôll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers, accelerate, peft, trl, and bitsandbytes libraries. This is what we‚Äôll do in the following code, based on Younes Belkada‚Äôs GitHub Gist. First, we install and load these libraries.\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\nLet‚Äôs talk a bit about the parameters we can tune here. First, we want to load a llama-2-7b-chat-hf model and train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model llama-2-7b-miniguanaco. If you‚Äôre interested in how this dataset was created, you can check this notebook. Feel free to change it: there are many good datasets on the Hugging Face Hub, like databricks/databricks-dolly-15k.\nQLoRA will use a rank of 64 with a scaling parameter of 16 (see this article for more information about LoRA parameters). We‚Äôll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for 1 epoch. To get more information about the other parameters, check the TrainingArguments, PeftModel, and SFTTrainer documentation.\n\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model name\nnew_model = \"llama-2-7b-miniguanaco\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n\nWe can now load everything and start the fine-tuning process. We‚Äôre relying on multiple wrappers, so bear with me.\n\nFirst of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\nThen, we‚Äôre configuring bitsandbytes for 4-bit quantization.\nNext, we‚Äôre loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\nFinally, we‚Äôre loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!\n\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)\n\n\n\n\nThe training can be very long, depending on the size of your dataset. Here, it took less than an hour on a T4 GPU. We can check the plots on tensorboard, as follows:\n\n%load_ext tensorboard\n%tensorboard --logdir results/runs\n\n\n\n\n\n\n\nLet‚Äôs make sure that the model is behaving correctly. It would require a more exhaustive evaluation, but we can use the text generation pipeline to ask questions like ‚ÄúWhat is a large language model?‚Äù Note that I‚Äôm formatting the input to match Llama 2‚Äôs prompt template.\n\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n\n\n&lt;s&gt;[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\n\n\nThe model outputs the following response:\nA large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\nFrom experience, it is very coherent for a model with only 7 billion parameters. You can play with it and ask harder questions from evaluation datasets like BigBench-Hard. Guanaco is an excellent dataset that has produced high-quality models in the past. You can train a Llama 2 model on the entire dataset using mlabonne/guanaco-llama2.\nHow can we store our new llama-2-7b-miniguanaco model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything. Alas, it also creates a problem with the VRAM (despite emptying it), so I recommend restarting the notebook, re-executing the three first cells, and then executing the next one. Please contact me if you know a fix!\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nOur weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.\n\n!huggingface-cli login\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mlabonne/llama-2-7b-guanaco/commit/0f5ed9581b805b659aec68484820edb5e3e6c3f5', commit_message='Upload tokenizer', commit_description='', oid='0f5ed9581b805b659aec68484820edb5e3e6c3f5', pr_url=None, pr_revision=None, pr_num=None)\n\n\nYou can now use this model for inference by loading it like any other Llama 2 model from the Hub. It is also possible to reload it for more fine-tuning ‚Äì perhaps with another dataset?\nIf you‚Äôre serious about fine-tuning models, using a script instead of a notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai, for less than 0.3$/h. Once you‚Äôre connected, you can install libraries, import your script, log in to Hugging Face and other tools (like Weights & Biases for logging your experiments), and start your fine-tuning.\nThe trl script is currently very limited, so I made my own based on the previous notebook. You can find it here on GitHub Gist. If you‚Äôre looking for a comprehensive solution, check out Axolotl from the OpenAccess AI Collective, which natively handles multiple datasets, Deepspeed, Flash Attention, etc.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we saw how to fine-tune a Llama 2 7b model using a Colab notebook. We introduced some necessary background on LLM training and fine-tuning, as well as important considerations related to instruction datasets. In the second section, we successfully fine-tuned the Llama 2 model with its native prompt template and custom parameters.\nThese fine-tuned models can then be integrated into LangChain and other architectures as advantageous alternatives to the OpenAI API. Remember, in this new paradigm, instruction datasets are the new gold, and the quality of your model heavily depends on the data on which it‚Äôs been fine-tuned. So, good luck with building high-quality datasets!\nIf you‚Äôre interested in more content about LLMs, follow me on Twitter @maximelabonne.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "References",
    "text": "References\n\nHugo Touvron, Thomas Scialom, et al.¬†(2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.\nPhilipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 is here - get it on Hugging Face. https://huggingface.co/blog/llama2\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, & Tatsunori B. Hashimoto. (2023). Stanford Alpaca: An Instruction-following LLaMA model.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, & Kristina Toutanova. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, & Luke Zettlemoyer. (2023). QLoRA: Efficient Finetuning of Quantized LLMs.",
    "crumbs": [
      "{{< fa pen >}} Articles",
      "üó£Ô∏è **Large language models**",
      "1. Fine-tune Llama 2 in Colab"
    ]
  }
]